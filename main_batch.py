#!/usr/bin/env python3
"""
äº¬ä¸œä»·æ ¼æ‰¹é‡çˆ¬å–å·¥å…·
æ”¯æŒåŸä»·å’Œä¿ƒé”€ä»·åŒä»·æ ¼æå–
ä½¿ç”¨æœç´¢æ–¹å¼ç»•è¿‡åçˆ¬æ£€æµ‹
"""
import pandas as pd
from datetime import datetime
import time
import random
import os
import re
from jd_crawler_via_search import JDCrawlerViaSearch


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("äº¬ä¸œä»·æ ¼æ‰¹é‡çˆ¬å–å·¥å…· - åŒä»·æ ¼ç‰ˆæœ¬")
    print("=" * 70)

    # æ£€æŸ¥cookies
    if not os.path.exists("jd_cookies.pkl"):
        print("\nâš ï¸  æœªæ‰¾åˆ°ç™»å½•å‡­æ®ï¼")
        print("è¯·å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤å®Œæˆé¦–æ¬¡ç™»å½•ï¼š")
        print("  python3 jd_crawler_via_search.py")
        return

    # æ–‡ä»¶è·¯å¾„
    input_file = "Product URL List.xlsx"
    output_file = "Price Marks.xlsx"
    sheet_name = "JD Top Model by Brand"

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(input_file):
        print(f"âœ— æ‰¾ä¸åˆ°æ–‡ä»¶: {input_file}")
        return

    # è¯»å–URLåˆ—è¡¨
    print(f"\næ­£åœ¨è¯»å– {input_file}...")
    try:
        df_urls = pd.read_excel(input_file, sheet_name=sheet_name)
        print(f"âœ“ æˆåŠŸè¯»å– {len(df_urls)} æ¡è®°å½•")
    except Exception as e:
        print(f"âœ— è¯»å–Excelå¤±è´¥: {str(e)}")
        return

    # è·å–URLåˆ—è¡¨
    if 'URL' not in df_urls.columns:
        print("âœ— æœªæ‰¾åˆ°'URL'åˆ—")
        return

    urls = df_urls['URL'].dropna().tolist()
    print(f"  æ‰¾åˆ° {len(urls)} ä¸ªæœ‰æ•ˆURL")

    # è¯¢é—®ç”¨æˆ·è¿è¡Œæ¨¡å¼
    print("\n" + "-" * 70)
    mode = input("é€‰æ‹©è¿è¡Œæ¨¡å¼:\n  1. æµ‹è¯•æ¨¡å¼ï¼ˆåªçˆ¬å–å‰3ä¸ªURLï¼‰\n  2. å°æ‰¹é‡æ¨¡å¼ï¼ˆçˆ¬å–å‰10ä¸ªURLï¼‰\n  3. å®Œæ•´æ¨¡å¼ï¼ˆçˆ¬å–å…¨éƒ¨URLï¼‰\nè¯·è¾“å…¥ 1ã€2 æˆ– 3: ").strip()

    if mode == "1":
        urls = urls[:3]
        print(f"\nâœ“ æµ‹è¯•æ¨¡å¼ï¼šå°†çˆ¬å–å‰3ä¸ªURL")
    elif mode == "2":
        urls = urls[:10]
        print(f"\nâœ“ å°æ‰¹é‡æ¨¡å¼ï¼šå°†çˆ¬å–å‰10ä¸ªURL")
    else:
        print(f"\nâœ“ å®Œæ•´æ¨¡å¼ï¼šå°†çˆ¬å–å…¨éƒ¨{len(urls)}ä¸ªURL")
        confirm = input(f"è¿™å°†èŠ±è´¹çº¦ {len(urls) * 5 / 60:.0f}-{len(urls) * 10 / 60:.0f} åˆ†é’Ÿï¼Œç¡®è®¤ç»§ç»­ï¼Ÿ(y/n): ").strip().lower()
        if confirm != 'y':
            print("å·²å–æ¶ˆ")
            return

    print("\n" + "-" * 70)

    # åˆå§‹åŒ–çˆ¬è™«ï¼ˆæœ‰çª—å£æ¨¡å¼ï¼Œæ–¹ä¾¿è§‚å¯Ÿï¼‰
    print("\nåˆå§‹åŒ–æµè§ˆå™¨...")
    crawler = JDCrawlerViaSearch(headless=False)

    try:
        # ç™»å½•
        print("ä½¿ç”¨ä¿å­˜çš„cookiesç™»å½•...")
        crawler.login()

        if not crawler.is_logged_in:
            print("\nâœ— ç™»å½•å¤±è´¥ï¼Œcookieså¯èƒ½å·²è¿‡æœŸ")
            print("è¯·è¿è¡Œ python3 jd_crawler_via_search.py é‡æ–°ç™»å½•")
            return

        print("âœ“ ç™»å½•æˆåŠŸï¼")

        # è®°å½•å½“å‰æ—¶é—´
        runtime = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # å¼€å§‹çˆ¬å–
        results = []
        success_count = 0
        failed_count = 0
        partial_count = 0  # åªè·å–åˆ°ä¸€ä¸ªä»·æ ¼

        print("\n" + "=" * 70)
        print("å¼€å§‹çˆ¬å–ä»·æ ¼")
        print("=" * 70 + "\n")

        for idx, url in enumerate(urls, 1):
            print(f"[{idx}/{len(urls)}] {url}")

            # å®šæœŸé‡å¯æµè§ˆå™¨ï¼ˆæ¯50ä¸ªå•†å“ï¼‰ï¼Œé¿å…å†…å­˜æ³„æ¼
            if idx > 1 and (idx - 1) % 50 == 0:
                print(f"\n  ğŸ”„ å·²å®Œæˆ {idx-1} ä¸ªå•†å“ï¼Œé‡å¯æµè§ˆå™¨é‡Šæ”¾å†…å­˜...\n")
                crawler.restart_browser()
                time.sleep(3)

            # æå–å•†å“ID
            match = re.search(r'/(\d+)\.html', url)
            if not match:
                print(f"  âœ— æ— æ³•æå–å•†å“ID\n")
                results.append({
                    'Runtime': runtime,
                    'URL': url,
                    'Price': 'N/A',
                    'Promotion Price': 'N/A'
                })
                failed_count += 1
                continue

            product_id = match.group(1)

            # è·å–ä»·æ ¼ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
            max_retries = 2
            retry_count = 0
            prices = None

            while retry_count <= max_retries:
                try:
                    # æ£€æŸ¥ä¼šè¯æ˜¯å¦æœ‰æ•ˆ
                    if not crawler.is_session_valid():
                        print(f"  âš ï¸  ä¼šè¯å¤±æ•ˆï¼Œå°è¯•é‡å¯æµè§ˆå™¨...")
                        if not crawler.restart_browser():
                            print(f"  âœ— æµè§ˆå™¨é‡å¯å¤±è´¥")
                            break
                        time.sleep(2)

                    prices = crawler.get_price_via_search(product_id)
                    break  # æˆåŠŸåˆ™é€€å‡ºé‡è¯•å¾ªç¯

                except Exception as e:
                    error_msg = str(e)
                    if "invalid session id" in error_msg.lower():
                        retry_count += 1
                        if retry_count <= max_retries:
                            print(f"  âš ï¸  ä¼šè¯å¤±æ•ˆï¼Œç¬¬ {retry_count} æ¬¡é‡è¯•...")
                            if crawler.restart_browser():
                                time.sleep(2)
                                continue
                            else:
                                print(f"  âœ— æµè§ˆå™¨é‡å¯å¤±è´¥")
                                break
                        else:
                            print(f"  âœ— é‡è¯• {max_retries} æ¬¡åä»å¤±è´¥")
                            break
                    else:
                        # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡º
                        raise

            # å¤„ç†ç»“æœ
            try:

                if prices:
                    original = prices.get('original')
                    promo = prices.get('promo')

                    # æ˜¾ç¤ºç»“æœ
                    if original and promo:
                        print(f"  âœ“ åŸä»·: Â¥{original}, ä¿ƒé”€ä»·: Â¥{promo}")
                        success_count += 1
                    elif original or promo:
                        if original:
                            print(f"  âš ï¸  åªæ‰¾åˆ°åŸä»·: Â¥{original}")
                        if promo:
                            print(f"  âš ï¸  åªæ‰¾åˆ°ä¿ƒé”€ä»·: Â¥{promo}")
                        partial_count += 1
                    else:
                        print(f"  âœ— æœªæ‰¾åˆ°ä»·æ ¼")
                        failed_count += 1

                    # ä¿å­˜ç»“æœ
                    results.append({
                        'Runtime': runtime,
                        'URL': url,
                        'Price': original if original else 'N/A',
                        'Promotion Price': promo if promo else 'N/A'
                    })
                else:
                    print(f"  âœ— è·å–å¤±è´¥")
                    failed_count += 1
                    results.append({
                        'Runtime': runtime,
                        'URL': url,
                        'Price': 'N/A',
                        'Promotion Price': 'N/A'
                    })

            except Exception as e:
                print(f"  âœ— é”™è¯¯: {e}")
                failed_count += 1
                results.append({
                    'Runtime': runtime,
                    'URL': url,
                    'Price': 'N/A',
                    'Promotion Price': 'N/A'
                })

            # æ·»åŠ å»¶è¿Ÿ
            if idx < len(urls):
                delay = random.uniform(3, 5)
                print(f"  ç­‰å¾… {delay:.1f} ç§’...\n")
                time.sleep(delay)

        # æ˜¾ç¤ºç»Ÿè®¡
        print("\n" + "=" * 70)
        print("çˆ¬å–å®Œæˆï¼")
        print("=" * 70)
        print(f"  å®Œå…¨æˆåŠŸï¼ˆä¸¤ä¸ªä»·æ ¼éƒ½è·å–ï¼‰: {success_count}")
        print(f"  éƒ¨åˆ†æˆåŠŸï¼ˆåªè·å–ä¸€ä¸ªä»·æ ¼ï¼‰: {partial_count}")
        print(f"  å¤±è´¥: {failed_count}")
        print(f"  æ€»è®¡: {len(urls)}")
        if len(urls) > 0:
            total_success = success_count + partial_count
            print(f"  æœ‰æ•ˆç‡: {total_success/len(urls)*100:.1f}%")

        # ä¿å­˜ç»“æœ
        print(f"\næ­£åœ¨ä¿å­˜ç»“æœåˆ° {output_file}...")

        try:
            from openpyxl import load_workbook
            from openpyxl.utils.dataframe import dataframe_to_rows

            # åˆ›å»ºæ–°æ•°æ®
            df_new = pd.DataFrame(results)

            if os.path.exists(output_file):
                # æ–‡ä»¶å­˜åœ¨ï¼Œè¿½åŠ æ•°æ®ï¼ˆä¿ç•™ Excel Table æ ¼å¼ï¼‰
                print(f"  æ£€æµ‹åˆ°ç°æœ‰æ–‡ä»¶ï¼Œè¿½åŠ æ–°æ•°æ®...")

                # åŠ è½½ç°æœ‰å·¥ä½œç°¿
                wb = load_workbook(output_file)

                # è·å– Marks sheet
                if 'Marks' in wb.sheetnames:
                    ws = wb['Marks']

                    # æ‰¾åˆ°è¡¨æ ¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    table = None
                    if ws.tables:
                        table_name = list(ws.tables.keys())[0]
                        table = ws.tables[table_name]
                        print(f"  å‘ç° Excel Table: {table_name}")

                    # æ‰¾åˆ°æœ€åä¸€è¡Œ
                    last_row = ws.max_row

                    # è¯»å–ç°æœ‰æ•°æ®è¡Œæ•°
                    existing_count = last_row - 1  # å‡å»è¡¨å¤´

                    # è¿½åŠ æ–°æ•°æ®ï¼ˆä»æœ€åä¸€è¡Œçš„ä¸‹ä¸€è¡Œå¼€å§‹ï¼‰
                    for r_idx, row in enumerate(dataframe_to_rows(df_new, index=False, header=False), start=last_row + 1):
                        for c_idx, value in enumerate(row, start=1):
                            ws.cell(row=r_idx, column=c_idx, value=value)

                    # å¦‚æœæœ‰ Tableï¼Œæ‰©å±•å…¶èŒƒå›´
                    if table:
                        # è®¡ç®—æ–°çš„è¡¨æ ¼èŒƒå›´
                        new_last_row = last_row + len(df_new)
                        # è·å–åˆ—æ•°
                        num_cols = len(df_new.columns)
                        # æ›´æ–°è¡¨æ ¼èŒƒå›´
                        from openpyxl.utils import get_column_letter
                        new_ref = f"A1:{get_column_letter(num_cols)}{new_last_row}"
                        table.ref = new_ref
                        print(f"  å·²æ‰©å±• Table èŒƒå›´åˆ°: {new_ref}")

                    # ä¿å­˜
                    wb.save(output_file)
                    print(f"âœ“ æˆåŠŸè¿½åŠ  {len(df_new)} æ¡æ–°è®°å½•")
                    print(f"  æ€»è®°å½•æ•°: {existing_count + len(df_new)}")

                else:
                    # Sheet ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„
                    df_new.to_excel(output_file, sheet_name='Marks', index=False)
                    print(f"âœ“ åˆ›å»ºæ–°è¡¨æ ¼ï¼Œä¿å­˜ {len(df_new)} æ¡è®°å½•")

            else:
                # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ–‡ä»¶
                df_new.to_excel(output_file, sheet_name='Marks', index=False)
                print(f"âœ“ åˆ›å»ºæ–°æ–‡ä»¶ï¼Œä¿å­˜ {len(df_new)} æ¡è®°å½•")

            # æ˜¾ç¤ºExcelåˆ—æ ¼å¼
            print("\nExcel åˆ—æ ¼å¼:")
            print("  1. Runtime - è¿è¡Œæ—¶é—´")
            print("  2. URL - å•†å“é“¾æ¥")
            print("  3. Price - åŸä»·")
            print("  4. Promotion Price - ä¿ƒé”€ä»·")

        except Exception as e:
            print(f"âœ— ä¿å­˜å¤±è´¥: {str(e)}")
            # ä¿å­˜åˆ°å¤‡ä»½æ–‡ä»¶
            backup_file = f"Price_Marks_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
            try:
                df_new.to_excel(backup_file, index=False)
                print(f"  å·²ä¿å­˜åˆ°å¤‡ä»½æ–‡ä»¶: {backup_file}")
            except:
                pass

        print("\n" + "=" * 70)
        print("ç¨‹åºç»“æŸ")
        print("=" * 70)

    except KeyboardInterrupt:
        print("\n\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nå‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        print("\næ­£åœ¨å…³é—­æµè§ˆå™¨...")
        crawler.close()
        print("âœ“ å·²å…³é—­")


if __name__ == "__main__":
    main()
