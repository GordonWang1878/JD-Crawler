#!/usr/bin/env python3
"""
äº¬ä¸œä»·æ ¼æ‰¹é‡çˆ¬å–å·¥å…·
æ”¯æŒåŸä»·å’Œä¿ƒé”€ä»·åŒä»·æ ¼æå–
ä½¿ç”¨æœç´¢æ–¹å¼ç»•è¿‡åçˆ¬æ£€æµ‹
"""
import warnings
# æŠ‘åˆ¶ urllib3 çš„ OpenSSL è­¦å‘Š
warnings.filterwarnings('ignore', message='urllib3 v2 only supports OpenSSL 1.1.1+')

import pandas as pd
from datetime import datetime
import time
import random
import os
import re
from jd_crawler_via_search import JDCrawlerViaSearch


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("äº¬ä¸œä»·æ ¼æ‰¹é‡çˆ¬å–å·¥å…· - åŒä»·æ ¼ç‰ˆæœ¬")
    print("=" * 70)

    # æ–‡ä»¶è·¯å¾„
    input_file = "Product URL List.xlsx"
    output_file = "Price Marks.xlsx"
    sheet_name = "JD Top Model by Brand"

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(input_file):
        print(f"âœ— æ‰¾ä¸åˆ°æ–‡ä»¶: {input_file}")
        return

    # è¯»å–URLåˆ—è¡¨
    print(f"\næ­£åœ¨è¯»å– {input_file}...")
    try:
        df_urls = pd.read_excel(input_file, sheet_name=sheet_name)
        print(f"âœ“ æˆåŠŸè¯»å– {len(df_urls)} æ¡è®°å½•")
    except Exception as e:
        print(f"âœ— è¯»å–Excelå¤±è´¥: {str(e)}")
        return

    # è·å–URLåˆ—è¡¨
    if 'URL' not in df_urls.columns:
        print("âœ— æœªæ‰¾åˆ°'URL'åˆ—")
        return

    urls = df_urls['URL'].dropna().tolist()
    print(f"  æ‰¾åˆ° {len(urls)} ä¸ªæœ‰æ•ˆURL")

    # è¯¢é—®ç”¨æˆ·è¿è¡Œæ¨¡å¼
    print("\n" + "-" * 70)
    mode = input("é€‰æ‹©è¿è¡Œæ¨¡å¼:\n  1. æµ‹è¯•æ¨¡å¼ï¼ˆåªçˆ¬å–å‰3ä¸ªURLï¼‰\n  2. å°æ‰¹é‡æ¨¡å¼ï¼ˆçˆ¬å–å‰10ä¸ªURLï¼‰\n  3. å®Œæ•´æ¨¡å¼ï¼ˆçˆ¬å–å…¨éƒ¨URLï¼‰\nè¯·è¾“å…¥ 1ã€2 æˆ– 3: ").strip()

    if mode == "1":
        urls = urls[:3]
        print(f"\nâœ“ æµ‹è¯•æ¨¡å¼ï¼šå°†çˆ¬å–å‰3ä¸ªURL")
    elif mode == "2":
        urls = urls[:10]
        print(f"\nâœ“ å°æ‰¹é‡æ¨¡å¼ï¼šå°†çˆ¬å–å‰10ä¸ªURL")
    else:
        print(f"\nâœ“ å®Œæ•´æ¨¡å¼ï¼šå°†çˆ¬å–å…¨éƒ¨{len(urls)}ä¸ªURL")
        confirm = input(f"è¿™å°†èŠ±è´¹çº¦ {len(urls) * 5 / 60:.0f}-{len(urls) * 10 / 60:.0f} åˆ†é’Ÿï¼Œç¡®è®¤ç»§ç»­ï¼Ÿ(y/n): ").strip().lower()
        if confirm != 'y':
            print("å·²å–æ¶ˆ")
            return

    print("\n" + "-" * 70)

    # åˆå§‹åŒ–çˆ¬è™«ï¼ˆæœ‰çª—å£æ¨¡å¼ï¼Œæ–¹ä¾¿è§‚å¯Ÿï¼‰
    print("\nåˆå§‹åŒ–æµè§ˆå™¨...")
    crawler = JDCrawlerViaSearch(headless=False)

    try:
        # ç™»å½•ï¼ˆä¼šè‡ªåŠ¨å¤„ç† cookies å¤±æ•ˆçš„æƒ…å†µï¼‰
        crawler.login()

        if not crawler.is_logged_in:
            print("\nâœ— ç™»å½•å¤±è´¥")
            return

        print("âœ“ ç™»å½•æˆåŠŸï¼")

        # è®°å½•æ‰¹æ¬¡å¼€å§‹æ—¶é—´
        batch_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        batch_start_timestamp = time.time()

        # å¼€å§‹çˆ¬å–
        results = []
        success_count = 0
        failed_count = 0
        unavailable_count = 0  # å·²ä¸‹æ¶çš„å•†å“
        total_item_time = 0  # ç´¯è®¡å¤„ç†æ—¶é—´

        print("\n" + "=" * 70)
        print("å¼€å§‹çˆ¬å–ä»·æ ¼")
        print("=" * 70 + "\n")

        for idx, url in enumerate(urls, 1):
            item_start_time = time.time()
            print(f"[{idx}/{len(urls)}] {url}")

            # å®šæœŸé‡å¯æµè§ˆå™¨ï¼ˆæ¯50ä¸ªå•†å“ï¼‰ï¼Œé¿å…å†…å­˜æ³„æ¼
            if idx > 1 and (idx - 1) % 50 == 0:
                print(f"\n  ğŸ”„ å·²å®Œæˆ {idx-1} ä¸ªå•†å“ï¼Œé‡å¯æµè§ˆå™¨é‡Šæ”¾å†…å­˜...\n")
                crawler.restart_browser()
                time.sleep(1.5)  # ä¼˜åŒ–åï¼š3s â†’ 2s â†’ 1.5s

            # æå–å•†å“ID
            match = re.search(r'/(\d+)\.html', url)
            if not match:
                print(f"  âœ— æ— æ³•æå–å•†å“ID\n")
                crawl_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                results.append({
                    'Batch Time': batch_time,
                    'Crawl Time': crawl_time,
                    'URL': url,
                    'Price': 'N/A',
                    'Promotion Price': 'N/A'
                })
                failed_count += 1
                continue

            product_id = match.group(1)

            # è·å–ä»·æ ¼ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
            max_retries = 2
            retry_count = 0
            prices = None

            while retry_count <= max_retries:
                try:
                    # æ£€æŸ¥ä¼šè¯æ˜¯å¦æœ‰æ•ˆ
                    if not crawler.is_session_valid():
                        print(f"  âš ï¸  ä¼šè¯å¤±æ•ˆï¼Œå°è¯•é‡å¯æµè§ˆå™¨...")
                        if not crawler.restart_browser():
                            print(f"  âœ— æµè§ˆå™¨é‡å¯å¤±è´¥")
                            break
                        time.sleep(1.0)  # ä¼˜åŒ–åï¼š2s â†’ 1.5s â†’ 1s

                    prices = crawler.get_price_via_search(product_id)
                    break  # æˆåŠŸåˆ™é€€å‡ºé‡è¯•å¾ªç¯

                except Exception as e:
                    error_msg = str(e)
                    if "invalid session id" in error_msg.lower():
                        retry_count += 1
                        if retry_count <= max_retries:
                            print(f"  âš ï¸  ä¼šè¯å¤±æ•ˆï¼Œç¬¬ {retry_count} æ¬¡é‡è¯•...")
                            if crawler.restart_browser():
                                time.sleep(1.0)  # ä¼˜åŒ–åï¼š2s â†’ 1.5s â†’ 1s
                                continue
                            else:
                                print(f"  âœ— æµè§ˆå™¨é‡å¯å¤±è´¥")
                                break
                        else:
                            print(f"  âœ— é‡è¯• {max_retries} æ¬¡åä»å¤±è´¥")
                            break
                    else:
                        # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡º
                        raise

            # å¤„ç†ç»“æœ
            try:

                if prices:
                    original = prices.get('original')
                    promo = prices.get('promo')

                    # æ£€æŸ¥å•†å“æ˜¯å¦ä¸‹æ¶
                    if original == 'unavailable' and promo == 'unavailable':
                        print(f"  âš ï¸  å•†å“å·²ä¸‹æ¶")
                        crawl_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        results.append({
                            'Batch Time': batch_time,
                            'Crawl Time': crawl_time,
                            'URL': url,
                            'Price': 'Unavailable',
                            'Promotion Price': 'Unavailable'
                        })
                        unavailable_count += 1
                        continue

                    # æ£€æŸ¥å•†å“æ˜¯å¦ä¸å­˜åœ¨ï¼ˆçœŸå®æ— æ³•è®¿é—®ï¼‰
                    if original == 'not_found' and promo == 'not_found':
                        print(f"  âš ï¸  å•†å“ä¸å­˜åœ¨")
                        crawl_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        results.append({
                            'Batch Time': batch_time,
                            'Crawl Time': crawl_time,
                            'URL': url,
                            'Price': 'Not Found',
                            'Promotion Price': 'Not Found'
                        })
                        unavailable_count += 1
                        continue

                    # æ£€æŸ¥æ˜¯å¦è§¦å‘åçˆ¬éªŒè¯ï¼ˆå¯é‡è¯•ï¼‰
                    if original == 'blocked' and promo == 'blocked':
                        print(f"  âš ï¸  è§¦å‘åçˆ¬éªŒè¯ (å»ºè®®é‡è¯•)")
                        crawl_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        results.append({
                            'Batch Time': batch_time,
                            'Crawl Time': crawl_time,
                            'URL': url,
                            'Price': 'Blocked (Retry)',
                            'Promotion Price': 'Blocked (Retry)'
                        })
                        failed_count += 1
                        continue

                    # æ£€æŸ¥æ˜¯å¦403ç¦æ­¢è®¿é—®ï¼ˆåçˆ¬æ‹¦æˆªï¼Œå¯é‡è¯•ï¼‰
                    if original == 'forbidden' and promo == 'forbidden':
                        print(f"  âš ï¸  403ç¦æ­¢è®¿é—® (å»ºè®®é‡è¯•)")
                        crawl_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        results.append({
                            'Batch Time': batch_time,
                            'Crawl Time': crawl_time,
                            'URL': url,
                            'Price': 'Forbidden (Retry)',
                            'Promotion Price': 'Forbidden (Retry)'
                        })
                        failed_count += 1
                        continue

                    # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªä»·æ ¼ï¼Œä¸”ä¸¤ä¸ªå­—æ®µéƒ½æœ‰å€¼
                    # å•†å“ä¸€å®šæœ‰å¸¸è§„ä»·æ ¼ï¼Œå¯èƒ½æ²¡æœ‰ä¿ƒé”€ä»·
                    if original and promo:
                        # æœ€ç†æƒ³çš„æƒ…å†µï¼šä¸¤ä¸ªä»·æ ¼éƒ½æœ‰
                        print(f"  âœ“ åŸä»·: Â¥{original}, ä¿ƒé”€ä»·: Â¥{promo}")
                        success_count += 1
                    elif original and not promo:
                        # åªæœ‰åŸä»·ï¼Œè¯´æ˜æ— ä¿ƒé”€ï¼Œä¸¤ä¸ªå­—æ®µå†™ç›¸åŒä»·æ ¼
                        print(f"  âœ“ åŸä»·: Â¥{original} (æ— ä¿ƒé”€)")
                        promo = original
                        success_count += 1
                    elif promo and not original:
                        # åªæ‰¾åˆ°ä¿ƒé”€ä»·ï¼Œå®é™…ä¸Šè¿™åº”è¯¥æ˜¯å¸¸è§„ä»·æ ¼
                        print(f"  âœ“ ä»·æ ¼: Â¥{promo} (ä½œä¸ºåŸä»·å’Œä¿ƒé”€ä»·)")
                        original = promo
                        success_count += 1
                    else:
                        print(f"  âœ— æœªæ‰¾åˆ°ä»·æ ¼ (å¯é‡è¯•)")
                        failed_count += 1

                    # ä¿å­˜ç»“æœï¼ˆç°åœ¨ original å’Œ promo è¦ä¹ˆéƒ½æœ‰å€¼ï¼Œè¦ä¹ˆéƒ½æ˜¯ Noneï¼‰
                    crawl_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    results.append({
                        'Batch Time': batch_time,
                        'Crawl Time': crawl_time,
                        'URL': url,
                        'Price': original if original else 'N/A (Retry)',
                        'Promotion Price': promo if promo else 'N/A (Retry)'
                    })
                else:
                    print(f"  âœ— è·å–å¤±è´¥ (å¯é‡è¯•)")
                    failed_count += 1
                    crawl_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    results.append({
                        'Batch Time': batch_time,
                        'Crawl Time': crawl_time,
                        'URL': url,
                        'Price': 'N/A (Retry)',
                        'Promotion Price': 'N/A (Retry)'
                    })

            except Exception as e:
                print(f"  âœ— é”™è¯¯: {e} (å¯é‡è¯•)")
                failed_count += 1
                crawl_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                results.append({
                    'Batch Time': batch_time,
                    'Crawl Time': crawl_time,
                    'URL': url,
                    'Price': 'N/A (Retry)',
                    'Promotion Price': 'N/A (Retry)'
                })

            # è®¡ç®—å’Œæ˜¾ç¤ºæ—¶é—´ä¿¡æ¯
            item_elapsed = time.time() - item_start_time
            total_item_time += item_elapsed
            avg_time_per_item = total_item_time / idx
            remaining_items = len(urls) - idx
            estimated_remaining_time = avg_time_per_item * remaining_items

            print(f"  â±ï¸  æœ¬å•†å“ç”¨æ—¶: {item_elapsed:.1f}ç§’ | å¹³å‡: {avg_time_per_item:.1f}ç§’/å•†å“", end="")
            if remaining_items > 0:
                minutes = int(estimated_remaining_time // 60)
                seconds = int(estimated_remaining_time % 60)
                print(f" | é¢„è®¡å‰©ä½™: {minutes}åˆ†{seconds}ç§’")
            else:
                print()

            # æ·»åŠ å»¶è¿Ÿ
            if idx < len(urls):
                delay = random.uniform(1.5, 2.5)  # ä¼˜åŒ–åï¼š3-5s â†’ 2-3s â†’ 1.5-2.5s
                print(f"  ç­‰å¾… {delay:.1f} ç§’...\n")
                time.sleep(delay)

        # æ˜¾ç¤ºç»Ÿè®¡
        total_elapsed = time.time() - batch_start_timestamp
        total_minutes = int(total_elapsed // 60)
        total_seconds = int(total_elapsed % 60)

        print("\n" + "=" * 70)
        print("çˆ¬å–å®Œæˆï¼")
        print("=" * 70)
        print(f"  æˆåŠŸ: {success_count}")
        print(f"  å·²ä¸‹æ¶: {unavailable_count}")
        print(f"  å¤±è´¥: {failed_count}")
        print(f"  æ€»è®¡: {len(urls)}")
        if len(urls) > 0:
            print(f"  æˆåŠŸç‡: {success_count/len(urls)*100:.1f}%")
            if unavailable_count > 0:
                print(f"  ä¸‹æ¶ç‡: {unavailable_count/len(urls)*100:.1f}%")
        print(f"\n  â±ï¸  æ€»ç”¨æ—¶: {total_minutes}åˆ†{total_seconds}ç§’")
        if len(urls) > 0:
            avg_per_item = total_elapsed / len(urls)
            print(f"  å¹³å‡: {avg_per_item:.1f}ç§’/å•†å“")

        # ä¿å­˜ç»“æœ
        print(f"\næ­£åœ¨ä¿å­˜ç»“æœåˆ° {output_file}...")

        try:
            from openpyxl import load_workbook
            from openpyxl.utils.dataframe import dataframe_to_rows

            # åˆ›å»ºæ–°æ•°æ®
            df_new = pd.DataFrame(results)

            if os.path.exists(output_file):
                # æ–‡ä»¶å­˜åœ¨ï¼Œè¿½åŠ æ•°æ®ï¼ˆä¿ç•™ Excel Table æ ¼å¼ï¼‰
                print(f"  æ£€æµ‹åˆ°ç°æœ‰æ–‡ä»¶ï¼Œè¿½åŠ æ–°æ•°æ®...")

                # åŠ è½½ç°æœ‰å·¥ä½œç°¿
                wb = load_workbook(output_file)

                # è·å– Marks sheet
                if 'Marks' in wb.sheetnames:
                    ws = wb['Marks']

                    # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿ç§»æ—§æ ¼å¼ï¼ˆRuntime â†’ Batch Time + Crawl Timeï¼‰
                    header_row = [cell.value for cell in ws[1]]

                    if 'Runtime' in header_row and 'Batch Time' not in header_row:
                        print(f"  æ£€æµ‹åˆ°æ—§æ ¼å¼ (Runtime åˆ—)ï¼Œè‡ªåŠ¨è¿ç§»...")

                        # æ‰¾åˆ° Runtime åˆ—çš„ä½ç½®
                        runtime_col_idx = header_row.index('Runtime') + 1  # openpyxl ä»1å¼€å§‹

                        # 1. å°† "Runtime" æ”¹ä¸º "Batch Time"
                        ws.cell(row=1, column=runtime_col_idx, value='Batch Time')
                        print(f"    âœ“ 'Runtime' â†’ 'Batch Time'")

                        # 2. åœ¨ Batch Time å³è¾¹æ’å…¥æ–°åˆ— "Crawl Time"
                        ws.insert_cols(runtime_col_idx + 1)
                        ws.cell(row=1, column=runtime_col_idx + 1, value='Crawl Time')
                        print(f"    âœ“ æ’å…¥ 'Crawl Time' åˆ—")

                        # 3. å¤åˆ¶ Batch Time çš„å€¼åˆ° Crawl Timeï¼ˆæ—§æ•°æ®æ²¡æœ‰åˆ†å¼€è®°å½•ï¼‰
                        for row_idx in range(2, ws.max_row + 1):
                            batch_time_value = ws.cell(row=row_idx, column=runtime_col_idx).value
                            ws.cell(row=row_idx, column=runtime_col_idx + 1, value=batch_time_value)
                        print(f"    âœ“ å·²å¡«å…… {ws.max_row - 1} è¡Œæ•°æ®çš„ Crawl Time")

                        # 4. å¦‚æœæœ‰ Tableï¼Œæ›´æ–°å…¶èŒƒå›´ï¼ˆå¤šäº†ä¸€åˆ—ï¼‰
                        if ws.tables:
                            table_name = list(ws.tables.keys())[0]
                            table = ws.tables[table_name]
                            # è·å–æ–°çš„åˆ—æ•°
                            from openpyxl.utils import get_column_letter
                            new_col_count = len(header_row) + 1  # å¤šäº†ä¸€åˆ—
                            new_ref = f"A1:{get_column_letter(new_col_count)}{ws.max_row}"
                            table.ref = new_ref
                            print(f"    âœ“ æ›´æ–° Table èŒƒå›´: {new_ref}")

                        # 5. ä¿å­˜è¿ç§»åçš„æ–‡ä»¶
                        wb.save(output_file)
                        print(f"  âœ“ åˆ—æ ¼å¼è¿ç§»å®Œæˆï¼")

                        # é‡æ–°åŠ è½½å·¥ä½œç°¿ï¼ˆç¡®ä¿åç»­æ“ä½œä½¿ç”¨æ›´æ–°åçš„ç»“æ„ï¼‰
                        wb = load_workbook(output_file)
                        ws = wb['Marks']

                    # æ‰¾åˆ°è¡¨æ ¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    table = None
                    if ws.tables:
                        table_name = list(ws.tables.keys())[0]
                        table = ws.tables[table_name]
                        print(f"  å‘ç° Excel Table: {table_name}")

                    # æ‰¾åˆ°æœ€åä¸€è¡Œ
                    last_row = ws.max_row

                    # è¯»å–ç°æœ‰æ•°æ®è¡Œæ•°
                    existing_count = last_row - 1  # å‡å»è¡¨å¤´

                    # è¿½åŠ æ–°æ•°æ®ï¼ˆä»æœ€åä¸€è¡Œçš„ä¸‹ä¸€è¡Œå¼€å§‹ï¼‰
                    for r_idx, row in enumerate(dataframe_to_rows(df_new, index=False, header=False), start=last_row + 1):
                        for c_idx, value in enumerate(row, start=1):
                            ws.cell(row=r_idx, column=c_idx, value=value)

                    # å¦‚æœæœ‰ Tableï¼Œæ‰©å±•å…¶èŒƒå›´
                    if table:
                        # è®¡ç®—æ–°çš„è¡¨æ ¼èŒƒå›´
                        new_last_row = last_row + len(df_new)
                        # è·å–åˆ—æ•°
                        num_cols = len(df_new.columns)
                        # æ›´æ–°è¡¨æ ¼èŒƒå›´
                        from openpyxl.utils import get_column_letter
                        new_ref = f"A1:{get_column_letter(num_cols)}{new_last_row}"
                        table.ref = new_ref
                        print(f"  å·²æ‰©å±• Table èŒƒå›´åˆ°: {new_ref}")

                    # ä¿å­˜
                    wb.save(output_file)
                    print(f"âœ“ æˆåŠŸè¿½åŠ  {len(df_new)} æ¡æ–°è®°å½•")
                    print(f"  æ€»è®°å½•æ•°: {existing_count + len(df_new)}")

                else:
                    # Sheet ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„
                    df_new.to_excel(output_file, sheet_name='Marks', index=False)
                    print(f"âœ“ åˆ›å»ºæ–°è¡¨æ ¼ï¼Œä¿å­˜ {len(df_new)} æ¡è®°å½•")

            else:
                # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ–‡ä»¶
                df_new.to_excel(output_file, sheet_name='Marks', index=False)
                print(f"âœ“ åˆ›å»ºæ–°æ–‡ä»¶ï¼Œä¿å­˜ {len(df_new)} æ¡è®°å½•")

            # æ˜¾ç¤ºExcelåˆ—æ ¼å¼
            print("\nExcel åˆ—æ ¼å¼:")
            print("  1. Batch Time - æ‰¹æ¬¡å¼€å§‹æ—¶é—´")
            print("  2. Crawl Time - å•†å“çˆ¬å–å®Œæˆæ—¶é—´")
            print("  3. URL - å•†å“é“¾æ¥")
            print("  4. Price - åŸä»·")
            print("  5. Promotion Price - ä¿ƒé”€ä»·")

        except Exception as e:
            print(f"âœ— ä¿å­˜å¤±è´¥: {str(e)}")
            # ä¿å­˜åˆ°å¤‡ä»½æ–‡ä»¶
            backup_file = f"Price_Marks_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
            try:
                df_new.to_excel(backup_file, index=False)
                print(f"  å·²ä¿å­˜åˆ°å¤‡ä»½æ–‡ä»¶: {backup_file}")
            except:
                pass

        print("\n" + "=" * 70)
        print("ç¨‹åºç»“æŸ")
        print("=" * 70)

    except KeyboardInterrupt:
        print("\n\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nå‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        print("\næ­£åœ¨å…³é—­æµè§ˆå™¨...")
        crawler.close()
        print("âœ“ å·²å…³é—­")


if __name__ == "__main__":
    main()
